{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc4f502-2e61-4c76-9d43-d3c83246c2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (66.0.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
      "Requirement already satisfied: wheel in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (0.38.4)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.38.4\n",
      "    Uninstalling wheel-0.38.4:\n",
      "      Successfully uninstalled wheel-0.38.4\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 66.0.0\n",
      "    Uninstalling setuptools-66.0.0:\n",
      "      Successfully uninstalled setuptools-66.0.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-24.0 setuptools-69.2.0 wheel-0.43.0\n",
      "Collecting spacy[apple]\n",
      "  Downloading spacy-3.7.4-cp310-cp310-macosx_10_9_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy[apple])\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy[apple])\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy[apple])\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy[apple])\n",
      "  Downloading cymem-2.0.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy[apple])\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy[apple])\n",
      "  Downloading thinc-8.2.3-cp310-cp310-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy[apple])\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy[apple])\n",
      "  Downloading srsly-2.4.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy[apple])\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy[apple])\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy[apple])\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy[apple]) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy[apple]) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy[apple]) (2.29.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy[apple])\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy[apple]) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy[apple]) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy[apple]) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy[apple])\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy[apple]) (1.26.4)\n",
      "Collecting thinc-apple-ops<1.0.0,>=0.1.0.dev0 (from spacy[apple])\n",
      "  Downloading thinc_apple_ops-0.1.4-cp310-cp310-macosx_10_9_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[apple])\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[apple])\n",
      "  Downloading pydantic_core-2.16.3-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[apple])\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy[apple]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy[apple]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy[apple]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy[apple]) (2023.5.7)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy[apple])\n",
      "  Downloading blis-0.7.11-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy[apple])\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy[apple]) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy[apple])\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from jinja2->spacy[apple]) (2.1.1)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-macosx_10_9_x86_64.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp310-cp310-macosx_10_9_x86_64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-macosx_10_9_x86_64.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.2/132.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-macosx_10_9_x86_64.whl (492 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.3-cp310-cp310-macosx_10_9_x86_64.whl (872 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m872.6/872.6 kB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc_apple_ops-0.1.4-cp310-cp310-macosx_10_9_x86_64.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.7.4-cp310-cp310-macosx_10_9_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, langcodes, catalogue, blis, annotated-types, typer, srsly, pydantic-core, preshed, cloudpathlib, pydantic, confection, weasel, thinc, thinc-apple-ops, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.4 pydantic-core-2.16.3 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 thinc-apple-ops-0.1.4 typer-0.9.4 typing-extensions-4.10.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sofiawang/anaconda3/envs/dojo-env/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U 'spacy[apple]'\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca3a03f-aa87-404e-9ded-53a2e93eeadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1380794e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57aa20af-4b58-4405-9629-023c78fa5a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steps in the pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb045ba1-9339-4e5d-abda-ee4f32114e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable ner\n",
    "nlp_no_ner = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "# Print active components\n",
    "nlp_no_ner.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148de319-5359-4739-a2c7-1c12df98fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While running in Central Park, \n",
      "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\n"
     ]
    }
   ],
   "source": [
    "# define text for demonstration\n",
    "sample_text = \"While running in Central Park, \\nI noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\"\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f81b2c-4200-47b3-a7b1-efaa0146359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a doc with the nlp pipeline\n",
    "doc = nlp(sample_text)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9519b8ff-e288-4fae-b364-793240a047e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While running in Central Park, \n",
      "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "While running in Central Park, \n",
       "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82ad770-5542-459c-84e3-0757c627e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc objects contain Token objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91b59be7-2a6c-4db1-9a96-0d58e34401eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While\n",
      "running\n",
      "in\n",
      "Central\n",
      "Park\n",
      ",\n",
      "\n",
      "\n",
      "I\n",
      "noticed\n",
      "a\n",
      "discarded\n",
      "McDonald\n",
      "'s\n",
      "container\n",
      ",\n",
      "surounded\n",
      "by\n",
      "buzzing\n",
      "flies\n",
      "was\n",
      "annoying\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Printing the first 10 tokens separately\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b3a461-cffe-4a46-bad7-452edcc5c62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "running"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing a token from the doc\n",
    "token = doc[1]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b99ad9-cbb0-4968-9980-3c41abd5b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a)token.text: The original form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc2e67b-cd57-4bf0-9d0f-e86abf750869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390b6d5-8962-4128-b397-6d4bec3cdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) token.lemma_: The base or root form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0cf9778-e562-4557-a6b3-d630243aacac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75e6ea-d8d3-4c57-8aae-2a52f9571024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) token.pos_: The part-of-speech tag associated with the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d758bee-928f-4973-8e4b-1a83c1efc11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf189125-24a7-4314-8b74-abb1d4ff4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d) token.is_stop: Boolean flag to check if the token is a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d98f2a5-f95c-4a43-98b4-9f4c4efed4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cc8f0-a812-45c8-b31b-267e1b6cdd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e) token.is_punct: Boolean flag to check if the token is punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82e00944-cc6b-4f1f-a9f6-6578695aeb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9c33c-1949-408b-8e58-507a06d20edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f) token.is_space: Boolean flag to check if the token is a whitespace character (.e.g new line \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35589bf7-a04d-44ad-8eef-a14989ad9c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(token.is_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d103089-5285-4af6-9f2c-20856522ff9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.text</th>\n",
       "      <th>.lemma_</th>\n",
       "      <th>.pos_</th>\n",
       "      <th>.is_stop</th>\n",
       "      <th>.is_punct</th>\n",
       "      <th>.is_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>While</td>\n",
       "      <td>while</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>VERB</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Central</td>\n",
       "      <td>Central</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Park</td>\n",
       "      <td>Park</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noticed</td>\n",
       "      <td>notice</td>\n",
       "      <td>VERB</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     .text  .lemma_  .pos_  .is_stop  .is_punct  .is_space\n",
       "0    While    while  SCONJ      True      False      False\n",
       "1  running      run   VERB     False      False      False\n",
       "2       in       in    ADP      True      False      False\n",
       "3  Central  Central  PROPN     False      False      False\n",
       "4     Park     Park  PROPN     False      False      False\n",
       "5        ,        ,  PUNCT     False       True      False\n",
       "6       \\n       \\n  SPACE     False      False       True\n",
       "7        I        I   PRON      True      False      False\n",
       "8  noticed   notice   VERB     False      False      False\n",
       "9        a        a    DET      True      False      False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create dictionary for desired attributes for each token\n",
    "token_data = []\n",
    "for token in doc:\n",
    "    token_dict = {\n",
    "        \".text\": token.text,\n",
    "        \".lemma_\": token.lemma_,\n",
    "        \".pos_\": token.pos_,\n",
    "        \".is_stop\": token.is_stop,\n",
    "        \".is_punct\": token.is_punct,\n",
    "        \".is_space\": token.is_space\n",
    "    }\n",
    "    token_data.append(token_dict)\n",
    "# Save dictionary as a dataframe\n",
    "spacy_df = pd.DataFrame(token_data) \n",
    "spacy_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bd8b3e9-4ea1-4d9e-a046-499fe44e731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', ',', '\\n', 'noticed', 'discarded', 'mcdonald', 'container', ',', 'surounded', 'buzzing', 'flies', 'annoying', '.']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing with SpaCy\n",
    "\n",
    "# For loop to remove stopwords\n",
    "cleaned_tokens = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword, skip it\n",
    "    if token.is_stop == True:\n",
    "        continue \n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # keep the tokens'.text for the final list of tokens\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9bfec11-44bf-4002-bb4a-4d727b091014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_tokens = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    ##NEW: \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # keep the tokens'.text for the final list of tokens\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "        \n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa855ba1-77d8-44a5-b4af-3942bbe7dcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# Obtaining Lemmas\n",
    "\n",
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_lemmas = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # # keep the tokens'.text for the final list of tokens\n",
    "        # cleaned_tokens.append(token.text.lower())\n",
    "        # keep the tokens's .lemma_ for the final list of tokens\n",
    "        cleaned_lemmas.append(token.lemma_.lower())\n",
    "        \n",
    "print(cleaned_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b63c9a8-196e-439b-b612-6021f39340ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words:\n",
      " ['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying'] \n",
      "\n",
      "Lemmatized words:\n",
      " ['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# Compare text and lemmas\n",
    "print(\"Tokenized words:\\n\", cleaned_tokens,\"\\n\")\n",
    "print(\"Lemmatized words:\\n\", cleaned_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7c31d8c-650f-4673-8180-3f3bb21f767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionalizing Preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3734f023-88fa-4bc0-b73b-671468821fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc, remove_stopwords=True, remove_punct=True, use_lemmas=False):\n",
    "    \"\"\"Temporary Fucntion - for Education Purposes (we will make something better below)\n",
    "    \"\"\"\n",
    "    tokens = [ ]\n",
    "    for token in doc:\n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_stopwords == True) and (token.is_stop == True):\n",
    "            # Continue the loop with the next token\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_punct == True):\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_space == True):\n",
    "            continue\n",
    "    \n",
    "        ## Determine final form of output list of tokens/lemmas\n",
    "        if use_lemmas:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        else:\n",
    "            tokens.append(token.text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da96b697-b3a0-4cfe-914b-4908379f6e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', 'running', 'in', 'central', 'park', ',', '\\n', 'i', 'noticed', 'a', 'discarded', 'mcdonald', \"'s\", 'container', ',', 'surounded', 'by', 'buzzing', 'flies', 'was', 'annoying', '.']\n"
     ]
    }
   ],
   "source": [
    "# Convert the text to a doc.\n",
    "doc = nlp(sample_text)\n",
    "# Tokenizing, keeping stopwords and punctuatin\n",
    "dirty_tokens = preprocess_doc(doc, remove_stopwords=False,remove_punct=False)\n",
    "print(dirty_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5968208-1c31-4328-bfbf-e151725e3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing, removing stopwords and punctuation\n",
    "cleaned_tokens = preprocess_doc(doc, remove_stopwords=True,remove_punct=True)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "061aeec8-a804-4402-9014-38102717bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing, removing stopwords and punctuation\n",
    "cleaned_lemmas = preprocess_doc(doc, remove_stopwords=True,remove_punct=True, use_lemmas=True)\n",
    "print(cleaned_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a65f2b80-606e-4061-a053-84fdf8b022d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Batch Preprocessing with SpaCy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Example Framework (Not runnable)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m lists_of_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mtext1\u001b[49m, text2, text3]\n\u001b[1;32m      5\u001b[0m processed_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mpipe(list_of_texts):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text1' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch Preprocessing with SpaCy\n",
    "\n",
    "# Example Framework (Not runnable)\n",
    "lists_of_texts = [text1, text2, text3]\n",
    "processed_texts = []\n",
    "for doc in nlp.pipe(list_of_texts):\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        # ... the same logic from our preprocess docs function.\n",
    "        doc_tokens.append(token.text.lower())\n",
    "        \n",
    "    # Append the list of tokens for current doc to processed_texts\n",
    "    processed_texts.append(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfd4dca3-4734-48a5-8077-5d294c12315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess_texts(\n",
    "    texts,\n",
    "    nlp=None,\n",
    "    remove_stopwords=True,\n",
    "    remove_punct=True,\n",
    "    use_lemmas=False,\n",
    "    disable=[\"ner\"],\n",
    "    batch_size=50,\n",
    "    n_process=-1,\n",
    "):\n",
    "    \"\"\"Efficiently preprocess a collection of texts using nlp.pipe()\n",
    "    Args:\n",
    "        texts (collection of strings): collection of texts to process (e.g. df['text'])\n",
    "        nlp (spacy pipe), optional): Spacy nlp pipe. Defaults to None; if None, it creates a default 'en_core_web_sm' pipe.\n",
    "        remove_stopwords (bool, optional): Controls stopword removal. Defaults to True.\n",
    "        remove_punct (bool, optional): Controls punctuation removal. Defaults to True.\n",
    "        use_lemmas (bool, optional): lemmatize tokens. Defaults to False.\n",
    "        disable (list of strings, optional): named pipeline elements to disable. Defaults to [\"ner\"]: Used with nlp.pipe(disable=disable)\n",
    "        batch_size (int, optional): Number of texts to process in a batch. Defaults to 50.\n",
    "        n_process (int, optional): Number of CPU processors to use. Defaults to -1 (meaning all CPU cores).\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    # from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    processed_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=disable, batch_size=batch_size, n_process=n_process)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_stopwords == True) and (token.is_stop == True):\n",
    "                # Continue the loop with the next token\n",
    "                continue\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_punct == True):\n",
    "                continue\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_space == True):\n",
    "                continue\n",
    "            \n",
    "            ## Determine final form of output list of tokens/lemmas\n",
    "            if use_lemmas:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9123938b-505b-4a2e-8d84-2110a12e1374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:08,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Default args will produce tokens\n",
    "tokens = batch_preprocess_texts([sample_text])\n",
    "tokens = tokens[0]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eddd8fb8-b624-4558-9ed3-0c4fa93f216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:07,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting use_lemmas = True will produce lemmas\n",
    "lemmas = batch_preprocess_texts([sample_text], use_lemmas=True)\n",
    "lemmas = lemmas[0]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc57f981-ffe4-4399-83bd-17ebeeaad7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"While running in Central Park, I noticed that the constant buzzing of flies was annoying. However, I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\"\n",
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47170c22-8602-4a54-8f66-022a1112c429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting sentences from doc\n",
    "sentences = list(doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a49c6f0-3349-4758-b2fb-67c2e9f8fb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "While running in Central Park, I noticed that the constant buzzing of flies was annoying."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first sentence\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6354d6c8-79ec-4c89-9c2c-2d2776122e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central Park LOC\n",
      "McDonald ORG\n"
     ]
    }
   ],
   "source": [
    "# Print any named entities in the doc and its label\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d14aa3-c981-45e6-8f6e-f94fedbc0ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
