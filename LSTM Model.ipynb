{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967a5ae-e0d9-4301-884c-0af5d5d2b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbedff70-4d6e-4e85-afaa-9633fdd12eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data from your path\n",
    "df = pd.read_csv(\"Data/yelp-reviews-CA-2015-2018.csv.gz\", index_col = 'date', parse_dates=['date'])\n",
    "## Remove any non-english reviews\n",
    "df = df.loc[ df['language']=='en'].copy()\n",
    "# Keep only 1,3,5 star reviews\n",
    "df = df.loc[ df['stars'].isin([1,3,5])]\n",
    "# Take a smaller subset\n",
    "df = df.loc['2018']\n",
    "# Set the index\n",
    "df = df.set_index('review_id')\n",
    "# We will split on each space, and then get the length\n",
    "df['sequence_length'] =df['text'].map( lambda x: len(x.split(\" \")))\n",
    "# Define a filter to identify reviews less than 400 words\n",
    "filter_short = df['sequence_length']< 400\n",
    "# Keep short reviews (<400 words)\n",
    "df=  df.loc[filter_short]\n",
    "# Use RUS to reduce n to match minority group\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "df_ml,  _ = sampler.fit_resample(df, df['stars'])\n",
    "df_ml['stars'].value_counts()\n",
    "# Define X\n",
    "X = df_ml['text'].values\n",
    "# Create a map for targets\n",
    "target_map = {1:0,\n",
    "              3:1,\n",
    "              5:2}\n",
    "# Define y and apply the target_map\n",
    "y = df_ml['stars'].map(target_map)\n",
    "# Define classes variable\n",
    "classes = y.unique()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52169b3-0039-4c21-9602-2784111136e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert to Dataset Object\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((X, y))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Shuffle dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mshuffle(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(ds),reshuffle_each_iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert to Dataset Object\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "# Shuffle dataset\n",
    "ds = ds.shuffle(buffer_size=len(ds),reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf55cb6-ff6b-42f7-bcaa-7a34d8a91dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ratio of the train, validation, test split\n",
    "split_train = .7\n",
    "split_val =  .2\n",
    "split_test =  1 -( split_train + split_val )\n",
    "# Calculate the number of samples for training and validation data \n",
    "n_train_samples =  int(len(ds) * split_train)\n",
    "n_val_samples = int(len(ds) * split_val)\n",
    "n_test_samples = len(ds) -(n_train_samples + n_val_samples)\n",
    "# Set the batch size\n",
    "BATCH_SIZE =32\n",
    "import math\n",
    "# math.ceil will round up\n",
    "# How many batches? \n",
    "n_train_batches = math.ceil(n_train_samples/BATCH_SIZE)\n",
    "n_val_batches = math.ceil(n_val_samples/BATCH_SIZE)\n",
    "n_test_batches = math.ceil(n_test_samples/BATCH_SIZE)\n",
    "print(f\"    - train:\\t{n_train_samples} samples \\t({n_train_batches} batches)\")\n",
    "print(f\"    - val:  \\t{n_val_samples} samples \\t({n_val_batches} batches)\")\n",
    "print(f\"    - test: \\t{n_test_samples} samples \\t({n_test_batches} batches)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
